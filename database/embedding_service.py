"""
AIN Database Embedding Service - Step 4: Vector Fuel Integration
í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„ë² ë”© ì„œë¹„ìŠ¤ì˜ ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´ í†µí•© ëª¨ë“ˆ.

ì´ ëª¨ë“ˆì€ api/embedding.pyì˜ EmbeddingServiceë¥¼ ë˜í•‘í•˜ì—¬
LanceBridgeì™€ ì›í™œí•˜ê²Œ ì—°ë™ë˜ë„ë¡ ì¶”ê°€ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

Features:
- ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬ (ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆì— ë²¡í„°í™”)
- ìºì‹± ì§€ì› (ë™ì¼ í…ìŠ¤íŠ¸ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
- LanceBridgeì™€ì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤

Architecture:
    Nexus -> DatabaseEmbeddingService -> api/embedding.py -> Gemini API
                                      -> LanceBridge (Vector Storage)

Usage:
    from database.embedding_service import DatabaseEmbeddingService, get_db_embedding_service
    
    service = get_db_embedding_service()
    vector = service.embed_and_store("Hello, world!", memory_type="semantic")
    vectors = service.batch_embed(["text1", "text2", "text3"])
"""

import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# API Embedding Service ì„í¬íŠ¸
try:
    from api.embedding import EmbeddingService, get_embedding, HAS_GENAI
    HAS_EMBEDDING = True
except ImportError:
    HAS_EMBEDDING = False
    HAS_GENAI = False
    print("âš ï¸ api/embedding.py ì„í¬íŠ¸ ì‹¤íŒ¨. ì„ë² ë”© ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”.")

# LanceBridge ì„í¬íŠ¸
try:
    from database.lance_bridge import get_lance_bridge, LanceBridge, LANCE_AVAILABLE
    HAS_LANCE = LANCE_AVAILABLE
except ImportError:
    HAS_LANCE = False
    LanceBridge = None


class EmbeddingCache:
    """
    ì„ë² ë”© ìºì‹œ - ë™ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€
    
    ë©”ëª¨ë¦¬ ê¸°ë°˜ LRU ìºì‹œë¡œ, ì‹œìŠ¤í…œ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”ë¨.
    ì˜êµ¬ ìºì‹±ì´ í•„ìš”í•˜ë©´ LanceDBì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì¬ì‚¬ìš©.
    """
    
    MAX_CACHE_SIZE = 1000  # ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜
    
    def __init__(self):
        self._cache: Dict[str, List[float]] = {}
        self._access_order: List[str] = []  # LRU ì¶”ì 
    
    def _compute_key(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ í•´ì‹œ í‚¤ ìƒì„±"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
    
    def get(self, text: str) -> Optional[List[float]]:
        """ìºì‹œì—ì„œ ë²¡í„° ì¡°íšŒ"""
        key = self._compute_key(text)
        if key in self._cache:
            # LRU ì—…ë°ì´íŠ¸
            self._access_order.remove(key)
            self._access_order.append(key)
            return self._cache[key]
        return None
    
    def set(self, text: str, vector: List[float]):
        """ìºì‹œì— ë²¡í„° ì €ì¥"""
        key = self._compute_key(text)
        
        # ìºì‹œ í¬ê¸° ì œí•œ (LRU ë°©ì¶œ)
        if len(self._cache) >= self.MAX_CACHE_SIZE:
            oldest_key = self._access_order.pop(0)
            del self._cache[oldest_key]
        
        self._cache[key] = vector
        self._access_order.append(key)
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self._access_order.clear()
    
    def stats(self) -> Dict[str, int]:
        """ìºì‹œ í†µê³„"""
        return {
            "size": len(self._cache),
            "max_size": self.MAX_CACHE_SIZE
        }


class DatabaseEmbeddingService:
    """
    ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´ ì„ë² ë”© ì„œë¹„ìŠ¤
    
    api/embedding.pyì˜ EmbeddingServiceë¥¼ ë˜í•‘í•˜ì—¬
    ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬, LanceBridge í†µí•© ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.
    
    Attributes:
        embedding_service: ê¸°ë³¸ ì„ë² ë”© ì„œë¹„ìŠ¤ (Gemini API)
        lance_bridge: ë²¡í„° ì €ì¥ì†Œ (LanceDB)
        cache: ì„ë² ë”© ìºì‹œ
    """
    
    _instance: Optional["DatabaseEmbeddingService"] = None
    VECTOR_DIM = 384  # MiniLM ê¸°ì¤€ (LanceBridgeì™€ ë™ì¼)
    
    def __new__(cls):
        """ì‹±ê¸€í†¤ íŒ¨í„´"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        # ê¸°ë³¸ ì„ë² ë”© ì„œë¹„ìŠ¤
        self._embedding_service: Optional[EmbeddingService] = None
        if HAS_EMBEDDING:
            try:
                from api.embedding import EmbeddingService
                self._embedding_service = EmbeddingService()
            except Exception as e:
                print(f"âš ï¸ EmbeddingService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # LanceBridge ì—°ê²°
        self._lance_bridge: Optional[LanceBridge] = None
        if HAS_LANCE:
            try:
                self._lance_bridge = get_lance_bridge()
            except Exception as e:
                print(f"âš ï¸ LanceBridge ì—°ê²° ì‹¤íŒ¨: {e}")
        
        # ìºì‹œ ì´ˆê¸°í™”
        self._cache = EmbeddingCache()
        
        # í†µê³„
        self._stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "api_calls": 0,
            "stored_count": 0
        }
        
        self._initialized = True
        print("âœ… DatabaseEmbeddingService ì´ˆê¸°í™” ì™„ë£Œ")
    
    @property
    def is_available(self) -> bool:
        """ì„ë² ë”© ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self._embedding_service is not None
    
    @property
    def is_lance_connected(self) -> bool:
        """LanceBridge ì—°ê²° ì—¬ë¶€"""
        return self._lance_bridge is not None and self._lance_bridge.is_connected
    
    def embed(self, text: str, use_cache: bool = True) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ë³€í™˜í•  í…ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì„ë² ë”© ë²¡í„° (VECTOR_DIM ì°¨ì›)
        """
        self._stats["total_requests"] += 1
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached = self._cache.get(text)
            if cached is not None:
                self._stats["cache_hits"] += 1
                return cached
        
        # API í˜¸ì¶œ
        vector = self._call_embedding_api(text)
        
        # ìºì‹œ ì €ì¥
        if use_cache and vector:
            self._cache.set(text, vector)
        
        return vector
    
    def _call_embedding_api(self, text: str) -> List[float]:
        """ì‹¤ì œ ì„ë² ë”© API í˜¸ì¶œ"""
        self._stats["api_calls"] += 1
        
        if self._embedding_service:
            try:
                return self._embedding_service.embed(text)
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        
        # í´ë°±: í•´ì‹œ ê¸°ë°˜ ì˜ì‚¬ ë²¡í„°
        return self._generate_fallback_vector(text)
    
    def _generate_fallback_vector(self, text: str) -> List[float]:
        """API ì‹¤íŒ¨ ì‹œ í•´ì‹œ ê¸°ë°˜ ì˜ì‚¬ ë²¡í„° ìƒì„±"""
        import hashlib
        hash_bytes = hashlib.sha256(text.encode('utf-8')).digest()
        
        vector = []
        for i in range(self.VECTOR_DIM):
            byte_idx = i % len(hash_bytes)
            value = (hash_bytes[byte_idx] / 255.0) * 2 - 1  # -1 ~ 1 ë²”ìœ„
            vector.append(value)
        
        return vector
    
    def batch_embed(
        self, 
        texts: List[str], 
        use_cache: bool = True
    ) -> List[List[float]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë²¡í„° ë³€í™˜
        
        ìºì‹œ íˆíŠ¸ëœ í•­ëª©ì€ API í˜¸ì¶œì—ì„œ ì œì™¸í•˜ì—¬ ë¹„ìš© ì ˆê°.
        
        Args:
            texts: ë³€í™˜í•  í…ìŠ¤íŠ¸ ëª©ë¡
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì„ë² ë”© ë²¡í„° ëª©ë¡ (ì…ë ¥ ìˆœì„œ ìœ ì§€)
        """
        results: List[Optional[List[float]]] = [None] * len(texts)
        texts_to_embed: List[Tuple[int, str]] = []  # (index, text)
        
        # 1ë‹¨ê³„: ìºì‹œ í™•ì¸
        for i, text in enumerate(texts):
            self._stats["total_requests"] += 1
            
            if use_cache:
                cached = self._cache.get(text)
                if cached is not None:
                    self._stats["cache_hits"] += 1
                    results[i] = cached
                    continue
            
            texts_to_embed.append((i, text))
        
        # 2ë‹¨ê³„: ìºì‹œ ë¯¸ìŠ¤ëœ í•­ëª© API í˜¸ì¶œ
        for idx, text in texts_to_embed:
            vector = self._call_embedding_api(text)
            results[idx] = vector
            
            if use_cache:
                self._cache.set(text, vector)
        
        return results
    
    def embed_and_store(
        self,
        text: str,
        memory_type: str = "semantic",
        source: str = "embedding_service",
        metadata: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[float], bool]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  LanceDBì— ì €ì¥
        
        Args:
            text: ì €ì¥í•  í…ìŠ¤íŠ¸
            memory_type: ê¸°ì–µ ìœ í˜• (semantic, episodic, procedural)
            source: ê¸°ì–µ ì¶œì²˜
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
        Returns:
            (ë²¡í„°, ì €ì¥ ì„±ê³µ ì—¬ë¶€) íŠœí”Œ
        """
        # 1. ì„ë² ë”© ìƒì„±
        vector = self.embed(text)
        
        # 2. LanceDBì— ì €ì¥
        stored = False
        if self.is_lance_connected:
            try:
                stored = self._lance_bridge.add_memory(
                    text=text,
                    vector=vector,
                    memory_type=memory_type,
                    source=source,
                    metadata=metadata
                )
                if stored:
                    self._stats["stored_count"] += 1
            except Exception as e:
                print(f"âš ï¸ LanceDB ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return vector, stored
    
    def search_similar(
        self,
        text: str,
        limit: int = 5,
        memory_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬í•œ ê¸°ì–µ ê²€ìƒ‰
        
        Args:
            text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            limit: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            memory_type: íŠ¹ì • ê¸°ì–µ ìœ í˜•ìœ¼ë¡œ í•„í„°ë§
        
        Returns:
            ìœ ì‚¬í•œ ê¸°ì–µ ëª©ë¡
        """
        if not self.is_lance_connected:
            return []
        
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
        query_vector = self.embed(text)
        
        # LanceDB ê²€ìƒ‰
        try:
            return self._lance_bridge.search_memory(
                query_vector=query_vector,
                limit=limit,
                memory_type=memory_type
            )
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ ê¸°ì–µ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ í†µê³„ ë°˜í™˜"""
        cache_stats = self._cache.stats()
        
        return {
            **self._stats,
            "cache": cache_stats,
            "embedding_available": self.is_available,
            "lance_connected": self.is_lance_connected,
            "cache_hit_rate": (
                self._stats["cache_hits"] / max(self._stats["total_requests"], 1)
            ) * 100
        }
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        print("ğŸ—‘ï¸ ì„ë² ë”© ìºì‹œ ì´ˆê¸°í™”ë¨")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì ‘ê·¼ í—¬í¼
def get_db_embedding_service() -> DatabaseEmbeddingService:
    """ì „ì—­ DatabaseEmbeddingService ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return DatabaseEmbeddingService()


# í¸ì˜ í•¨ìˆ˜: ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© + ì €ì¥
def embed_and_store(
    text: str,
    memory_type: str = "semantic",
    source: str = "quick_embed"
) -> Tuple[List[float], bool]:
    """
    ë¹ ë¥¸ ì„ë² ë”© + ì €ì¥ í—¬í¼ í•¨ìˆ˜
    
    Usage:
        from database.embedding_service import embed_and_store
        vector, stored = embed_and_store("Hello, world!")
    """
    service = get_db_embedding_service()
    return service.embed_and_store(text, memory_type, source)