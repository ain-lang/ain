"""
Engine Meta Learner: ë©”íƒ€ì¸ì§€ í•™ìŠµ ë° ë³´ì • ëª¨ë“ˆ
Step 7: Meta-Cognition - Recursive Self-Optimization Loop

ì´ ëª¨ë“ˆì€ ì‹œìŠ¤í…œì˜ ì˜ˆì¸¡(Confidence)ê³¼ ì‹¤ì œ ê²°ê³¼(Outcome)ë¥¼ ë¹„êµ ë¶„ì„í•˜ì—¬,
ë©”íƒ€ì¸ì§€ í‰ê°€ ëª¨ë¸(MetaEvaluator)ì˜ ì •í™•ë„ë¥¼ ì§€ì†ì ìœ¼ë¡œ ë³´ì •(Calibration)í•œë‹¤.
'Recursive Self-Optimization' ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ í•µì‹¬ í”¼ë“œë°± ë£¨í”„ì´ë‹¤.

Architecture:
    Nexus (History)
        â†“ ì§„í™” ê¸°ë¡ (Metadataì— ì €ì¥ëœ Confidence vs Status)
    MetaLearner (ì´ ëª¨ë“ˆ)
        â†“ ë¶„ì„ (Overconfidence/Underconfidence ê°ì§€)
    Calibration Factor (ë³´ì • ê³„ìˆ˜)
        â†“
    FactCore (ì €ì¥ ë° ê³µìœ )

Usage:
    from engine.meta_learner import MetaLearningMixin
    
    class AINCore(MetaLearningMixin, ...):
        pass
        
    ain = AINCore()
    calibration = await ain.run_meta_learning_cycle()
"""

from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
import statistics

try:
    from nexus import Nexus
    HAS_NEXUS = True
except ImportError:
    HAS_NEXUS = False
    Nexus = None

try:
    from fact_core import FactCore
    HAS_FACT_CORE = True
except ImportError:
    HAS_FACT_CORE = False
    FactCore = None


@dataclass
class CalibrationResult:
    """ë©”íƒ€ì¸ì§€ ë³´ì • ê²°ê³¼"""
    bias_score: float        # >0: ê³¼ì‹ (Overconfident), <0: ìì‹ ê° ë¶€ì¡±(Underconfident)
    calibration_factor: float # ë³´ì • ê³„ìˆ˜ (0.8 ~ 1.2)
    accuracy: float          # ì˜ˆì¸¡ ì •í™•ë„
    sample_size: int         # ë¶„ì„ëœ ìƒ˜í”Œ ìˆ˜
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "bias_score": self.bias_score,
            "calibration_factor": self.calibration_factor,
            "accuracy": self.accuracy,
            "sample_size": self.sample_size,
            "timestamp": self.timestamp.isoformat()
        }


class MetaLearner:
    """
    ë©”íƒ€ì¸ì§€ í•™ìŠµê¸°
    
    ê³¼ê±°ì˜ ì§„í™” ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ë©”íƒ€ì¸ì§€ ì‹œìŠ¤í…œì˜ í¸í–¥ì„ ê³„ì‚°í•˜ê³  ë³´ì •í•œë‹¤.
    
    Attributes:
        nexus: Nexus ì¸ìŠ¤í„´ìŠ¤ (ê¸°ë¡ ì €ì¥ì†Œ)
        min_samples: ë¶„ì„ì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        smoothing_factor: ë³´ì • ê³„ìˆ˜ ë³€í™” ì™„í™” ë¹„ìœ¨ (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)
    """
    
    MIN_SAMPLES_DEFAULT = 5
    SMOOTHING_FACTOR_DEFAULT = 0.5
    CALIBRATION_FACTOR_MIN = 0.8
    CALIBRATION_FACTOR_MAX = 1.2
    
    def __init__(
        self, 
        nexus: Optional[Nexus] = None,
        min_samples: int = MIN_SAMPLES_DEFAULT,
        smoothing_factor: float = SMOOTHING_FACTOR_DEFAULT
    ):
        self.nexus = nexus
        self.min_samples = min_samples
        self.smoothing_factor = smoothing_factor
        self._last_calibration: Optional[CalibrationResult] = None
        
    def analyze_confidence_accuracy(self, limit: int = 20) -> CalibrationResult:
        """
        ìµœê·¼ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ìì‹ ê° ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•œë‹¤.
        
        Args:
            limit: ë¶„ì„í•  ìµœëŒ€ ê¸°ë¡ ìˆ˜
            
        Returns:
            CalibrationResult: ë³´ì • ê²°ê³¼ (í¸í–¥, ë³´ì • ê³„ìˆ˜, ì •í™•ë„ ë“±)
        """
        if self.nexus is None:
            return CalibrationResult(
                bias_score=0.0,
                calibration_factor=1.0,
                accuracy=0.0,
                sample_size=0
            )

        # Nexusì—ì„œ ìµœê·¼ ì§„í™” ê¸°ë¡ ì¡°íšŒ
        history = self._get_history_with_confidence(limit)
        if not history:
            return CalibrationResult(
                bias_score=0.0,
                calibration_factor=1.0,
                accuracy=0.0,
                sample_size=0
            )

        predictions = []
        outcomes = []
        
        for record in history:
            confidence = record.get("confidence")
            status = record.get("status")
            
            if confidence is None or status is None:
                continue
            
            # ì˜ˆì¸¡ê°’ (0.0 ~ 1.0)
            predictions.append(float(confidence))
            
            # ì‹¤ì œ ê²°ê³¼ (ì„±ê³µ=1.0, ì‹¤íŒ¨=0.0)
            if status == "success":
                outcomes.append(1.0)
            elif status == "failed":
                outcomes.append(0.0)
            else:
                # pending ë“±ì€ ì œì™¸
                predictions.pop()
                continue
        
        if len(predictions) < self.min_samples:
            return CalibrationResult(
                bias_score=0.0,
                calibration_factor=1.0,
                accuracy=0.0,
                sample_size=len(predictions)
            )

        # í¸í–¥ ê³„ì‚° (ì˜ˆì¸¡ í‰ê·  - ì‹¤ì œ í‰ê· )
        avg_pred = statistics.mean(predictions)
        avg_actual = statistics.mean(outcomes)
        bias = avg_pred - avg_actual
        
        # ë³´ì • ê³„ìˆ˜ ê³„ì‚° (ë‹¨ìˆœ ì—­ë³´ì •, ë²”ìœ„ ì œí•œ)
        # ê³¼ì‹ (bias > 0)í•˜ë©´ ê³„ìˆ˜ë¥¼ ë‚®ì¶¤ (< 1.0)
        # ìì‹ ê° ë¶€ì¡±(bias < 0)í•˜ë©´ ê³„ìˆ˜ë¥¼ ë†’ì„ (> 1.0)
        raw_factor = 1.0 - (bias * self.smoothing_factor)
        calibration_factor = max(
            self.CALIBRATION_FACTOR_MIN,
            min(self.CALIBRATION_FACTOR_MAX, raw_factor)
        )
        
        # ì •í™•ë„ (MAEì˜ ì—­ìˆ˜ ê°œë…)
        errors = [abs(p - o) for p, o in zip(predictions, outcomes)]
        accuracy = 1.0 - statistics.mean(errors)
        
        result = CalibrationResult(
            bias_score=bias,
            calibration_factor=calibration_factor,
            accuracy=accuracy,
            sample_size=len(predictions)
        )
        
        self._last_calibration = result
        return result
    
    def _get_history_with_confidence(self, limit: int) -> List[Dict[str, Any]]:
        """
        Nexusì—ì„œ confidence_scoreê°€ í¬í•¨ëœ ê¸°ë¡ë§Œ ì¶”ì¶œí•œë‹¤.
        
        Args:
            limit: ìµœëŒ€ ê¸°ë¡ ìˆ˜
            
        Returns:
            confidenceì™€ statusê°€ í¬í•¨ëœ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
        """
        if self.nexus is None:
            return []
        
        try:
            # Nexus.get_recent_history() í˜¸ì¶œ
            raw_history = self.nexus.get_recent_history(limit=limit)
            if not raw_history:
                return []
            
            result = []
            for record in raw_history:
                # metadataì—ì„œ confidence_score ì¶”ì¶œ
                metadata = record.get("metadata", {})
                if isinstance(metadata, str):
                    # JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
                    try:
                        import json
                        metadata = json.loads(metadata)
                    except (json.JSONDecodeError, TypeError):
                        metadata = {}
                
                confidence = metadata.get("confidence_score")
                status = record.get("status")
                
                if confidence is not None and status is not None:
                    result.append({
                        "confidence": confidence,
                        "status": status,
                        "timestamp": record.get("timestamp"),
                        "file": record.get("file")
                    })
            
            return result
            
        except Exception as e:
            print(f"[MetaLearner] ê¸°ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_last_calibration(self) -> Optional[CalibrationResult]:
        """ë§ˆì§€ë§‰ ë³´ì • ê²°ê³¼ ë°˜í™˜"""
        return self._last_calibration
    
    def get_calibration_summary(self) -> str:
        """ë³´ì • ìƒíƒœ ìš”ì•½ ë¬¸ìì—´ ë°˜í™˜"""
        if self._last_calibration is None:
            return "No calibration data available"
        
        cal = self._last_calibration
        bias_desc = "overconfident" if cal.bias_score > 0 else "underconfident"
        
        return (
            f"Calibration: factor={cal.calibration_factor:.3f}, "
            f"bias={cal.bias_score:.3f} ({bias_desc}), "
            f"accuracy={cal.accuracy:.2%}, "
            f"samples={cal.sample_size}"
        )


class MetaLearningMixin:
    """
    AINCoreìš© ë©”íƒ€ í•™ìŠµ ë¯¹ìŠ¤ì¸
    
    AINCoreì— ìƒì†ë˜ì–´ ë©”íƒ€ì¸ì§€ í•™ìŠµ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.
    run_meta_learning_cycle()ì„ í†µí•´ ì£¼ê¸°ì ìœ¼ë¡œ ë©”íƒ€ì¸ì§€ ë³´ì •ì„ ìˆ˜í–‰í•œë‹¤.
    
    Required attributes from AINCore:
    """
    
    _meta_learner: Optional[MetaLearner] = None
    
    def _get_meta_learner(self) -> MetaLearner:
        """MetaLearner ì¸ìŠ¤í„´ìŠ¤ë¥¼ lazy-loadë¡œ ê°€ì ¸ì˜¨ë‹¤."""
        if self._meta_learner is None:
            nexus = getattr(self, "nexus", None)
            self._meta_learner = MetaLearner(nexus=nexus)
        return self._meta_learner
        
    async def run_meta_learning_cycle(self) -> Dict[str, Any]:
        """
        ë©”íƒ€ í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰
        
        ê³¼ê±° ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ë©”íƒ€ì¸ì§€ ë³´ì • ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ ,
        ê²°ê³¼ë¥¼ FactCoreì— ì €ì¥í•˜ì—¬ MetaEvaluatorê°€ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ í•œë‹¤.
        
        Returns:
            ë³´ì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        learner = self._get_meta_learner()
        result = learner.analyze_confidence_accuracy()
        
        # ê²°ê³¼ë¥¼ FactCoreì— ì €ì¥ (Identityì˜ ì¼ë¶€ë¡œ í†µí•©)
        fact_core = getattr(self, "fact_core", None)
        if fact_core is not None and result.sample_size > 0:
            calibration_data = {
                "factor": result.calibration_factor,
                "bias": result.bias_score,
                "accuracy": result.accuracy,
                "sample_size": result.sample_size,
                "last_updated": result.timestamp.isoformat()
            }
            
            try:
                # 'meta_calibration' ë…¸ë“œ ì—…ë°ì´íŠ¸ (ì—†ìœ¼ë©´ ìƒì„±ë¨)
                fact_core.add_fact("meta_calibration", calibration_data)
                
                bias_direction = "overconfident" if result.bias_score > 0 else "underconfident"
                print(
                    f"ğŸ§  Meta-Learning: Calibration Factor updated to "
                    f"{result.calibration_factor:.3f} "
                    f"(Bias: {result.bias_score:.3f}, {bias_direction})"
                )
            except Exception as e:
                print(f"[MetaLearning] FactCore ì €ì¥ ì‹¤íŒ¨: {e}")
        elif result.sample_size == 0:
            print("ğŸ§  Meta-Learning: ë¶„ì„í•  ìƒ˜í”Œì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            
        return {
            "calibration_factor": result.calibration_factor,
            "bias_score": result.bias_score,
            "accuracy": result.accuracy,
            "sample_size": result.sample_size,
            "summary": learner.get_calibration_summary()
        }
    
    def get_current_calibration_factor(self) -> float:
        """
        í˜„ì¬ ë³´ì • ê³„ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤.
        
        FactCoreì— ì €ì¥ëœ ê°’ì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’(1.0)ì„ ë°˜í™˜í•œë‹¤.
        
        Returns:
            ë³´ì • ê³„ìˆ˜ (0.8 ~ 1.2)
        """
        fact_core = getattr(self, "fact_core", None)
        if fact_core is not None:
            try:
                calibration_data = fact_core.get_fact("meta_calibration", default={})
                if isinstance(calibration_data, dict):
                    return calibration_data.get("factor", 1.0)
            except Exception:
                pass
        
        return 1.0
    
    def apply_calibration_to_confidence(self, raw_confidence: float) -> float:
        """
        ì›ì‹œ ìì‹ ê° ì ìˆ˜ì— ë³´ì • ê³„ìˆ˜ë¥¼ ì ìš©í•œë‹¤.
        
        MetaEvaluatorê°€ ì‚°ì¶œí•œ ìì‹ ê° ì ìˆ˜ë¥¼ ë³´ì •í•˜ì—¬
        ê³¼ì‹ /ìì‹ ê° ë¶€ì¡± í¸í–¥ì„ ì™„í™”í•œë‹¤.
        
        Args:
            raw_confidence: ì›ì‹œ ìì‹ ê° ì ìˆ˜ (0.0 ~ 1.0)
            
        Returns:
            ë³´ì •ëœ ìì‹ ê° ì ìˆ˜ (0.0 ~ 1.0 ë²”ìœ„ë¡œ í´ë¨í•‘)
        """
        factor = self.get_current_calibration_factor()
        
        # ë³´ì • ì ìš©: ê³¼ì‹ (factor < 1.0)ì´ë©´ ì ìˆ˜ë¥¼ ë‚®ì¶”ê³ ,
        # ìì‹ ê° ë¶€ì¡±(factor > 1.0)ì´ë©´ ì ìˆ˜ë¥¼ ë†’ì„
        calibrated = raw_confidence * factor
        
        # ë²”ìœ„ í´ë¨í•‘ (0.0 ~ 1.0)
        return max(0.0, min(1.0, calibrated))